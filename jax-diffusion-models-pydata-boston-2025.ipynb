{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Beginner's Friendly Introduction to Diffusion Models in JAX\n",
    "\n",
    "**Who Am I?**\n",
    "I am a Postdoctoral Researcher at the Center for Astrophysics | Harvard & Smithsonian where I research in the field X-Ray and Gamma-Ray Astronomy. I develop new AI, machine learning and statistical methods as well as scientific open source software for the analysis of astronomical low counts data. I'm also editor for the Astronomy and Astrophysics track of the Journal of Open Source Software [JOSS](https://joss.theoj.org).\n",
    "\n",
    "You can find out more about me at:\n",
    "\n",
    "- https://axeldonath.com\n",
    "- https://github.com/adonath\n",
    "\n",
    "\n",
    "This tutorial notebook is a beginner's friendly introduction to diffusion models in JAX. It is mostly focused on introducing the core concepts of [diffusion models](https://en.wikipedia.org/wiki/Diffusion_model) from first principles along with teaching the basics of [JAX](https://docs.jax.dev/en/latest/), a differentiable, GPU accelerated, array library for deep learning and scientific computing.\n",
    "\n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "- \\>2 years of experience with Python, [Numpy](https://numpy.org) and [Matplotlib](https://matplotlib.org)\n",
    "    - array operations, random number sampling and 1d/2d plotting, histograms\n",
    "- Undergraduate level knowledge of probability, statistics and linear algebra\n",
    "    - Gaussian distribution, variance and mean\n",
    "    - Probability density functions\n",
    "    - Maximum Likelihood Estimation\n",
    "    - ...\n",
    "- No prior knowledge of Diffusion Models nor JAX is required!\n",
    "\n",
    "\n",
    "## What are Diffusion Models?\n",
    "Diffusion models are a class of generative models that are based on the concept of iteratively removing noise from an image\n",
    "or other data. A very general way of thinking about diffusion models is to think of them as sampling algorithms, that sample\n",
    "from a fixed distribution. Where the distribution we sample from is defined by the distribution of the data we use for training them. \n",
    "\n",
    "![images/sde_schematic.jpg]\n",
    "\n",
    "However sampling from high dimensional distributions, such as the distribution of natural images, where each pixel is treated as an independent dimension, can be quite challenging. Diffusion models are a way to make this sampling process feasible and scalable.\n",
    "Diffusion model are mathematically well founded and draw many parallels to actual physical diffusion processes, \n",
    "denoising methods in image processing as well as differential equations.\n",
    "\n",
    "In this tutorial We will take a look at a specific type of diffusion models called Denoising Diffusion Probabilistic Models (DDPMs) introduced by [Ho et al. in 2020.](https://arxiv.org/abs/2006.11239). We will start with simple \"toy examples\" on 1d and 2d data to illustrate the method and then proceed to a more realistic case of actually sampling images. \n",
    "\n",
    "**Note**: I personally find that \"diffusion models\" are to a certain degree a misnomer, as the relevant part of the model is the reverse process, where the diffuson process is inverted. But we will learn about that later.\n",
    "\n",
    "## What is JAX?\n",
    "In a single sentence you can think of JAX as Numpy with support for GPUs and automatic differentiation.\n",
    "It also includes a just in time compilation (JIT) and function transforms, such as vectorization (np.vectorize)\n",
    "which is automatically lowered to hardware accelerated XLA instructions (StableHLO).\n",
    "\n",
    "TBD...\n",
    "\n",
    "\n",
    "## More Ressources \n",
    "\n",
    "An excellent intuitive introduction to the math behind diffusion models can be found in the following video series by\n",
    "[Deepia](https://www.youtube.com/@Deepia-ls2fo) on Youtube:\n",
    "\n",
    "- Denoising Autoencoders: https://youtu.be/0V96wE7lY4w?si=h82tsQOYRc7-Vwly\n",
    "- Langevin Algorithm: https://youtu.be/m0OTso2Dc2U?si=_Yld6zr2jkQEGuuI\n",
    "- Diffusion Models: https://youtu.be/EhndHhIvWWw?si=e_s1A9dyOF7NLCsg\n",
    "\n",
    "If you would like to prepare for this tutorial I would recommend to watch those videos in advance. They are each 15-30 minutes long and watching them in the given order is recommended.\n",
    "\n",
    "If you are interested in more details and different perspectives about diffusion models \n",
    "I recommend to checkout the following blog posts:\n",
    "\n",
    "- Key read: https://yang-song.net/blog/2021/score/\n",
    "- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice\n",
    "\n",
    "\n",
    "## Glossary\n",
    "- **Score Function**: The score function of a distribution is the gradient of the log-density function with respect to the input.\n",
    "- **Reverse Process**: The reverse process is the process of removing noise from an image / data.\n",
    "- **Forward Process**: The forward process is the process of adding noise to an image / data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up: the Diffusion Process\n",
    "\n",
    "Diffusion is typically a physical process where particles spread from regions of high concentration to regions of low concentration over time, driven by random motion. In nature, this can be seen in phenomena like the dispersal of ink in water or the flow of heat through a solid. Mathematically, diffusion is often modeled using stochastic processes or differential equations, capturing how randomness leads to gradual smoothing and homogenization of an initial state. In a more genral understanding diffusion can also include convection, meaning a systematic drift of the particles driven by an external force, such as wind, or stiring in a mixing process. \n",
    "\n",
    "We will first explore the process of diffusion on a simple example in JAX. Let's start with the relevant imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import numpy as jnp\n",
    "import jax\n",
    "from functools import partial\n",
    "from jax import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jax.numpy` namespace can be thought of as a \"drop-in\" replacemnt for numpy. It was designed to be as similar as possible to the standard `numpy` namespace. In almost all cases you will find the equivalent of some `np.<>` function in `jnp.<>`. Howevere there is one important difference: array objects in JAX are **not mutable**, meaning they cannot be modified in place. Something like `array[2:3] = 3`, **does not work** in JAX, for reasons that become clear later, when we dicuss just in time compilation (JIT). There is a handful  more of these differences which are documented in [\"JAX - The Sharp Bits\"](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n",
    "\n",
    "For this illustrative example we will work with a simple mixture of Gaussians as the inital distribution of the \"particles\" (I will use \"particles\" and \"samples\" synonymously). In JAX we can just use functionality in the `jnp.<>` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, norm, mu, sigma):\n",
    "    \"\"\"Single Gaussian distribution\"\"\"\n",
    "    return norm * jnp.exp(-0.5 * ((x - mu) / sigma) ** 2) / (sigma * jnp.sqrt(2 * np.pi))\n",
    "\n",
    "\n",
    "def gmm(x, norm, mu, sigma):\n",
    "    \"\"\"Gaussian Mixture Model\"\"\"\n",
    "    values = jnp.sum(gaussian(x, norm, mu, sigma), axis=0) / mu.shape[0]\n",
    "\n",
    "    # later we compute the gradient, which requires a returning a scalar value\n",
    "    if values.shape == (1,):\n",
    "        return values[0]\n",
    "    \n",
    "    return values\n",
    "\n",
    "norm, mu, sigma = jnp.array([1, 1])[:, None], jnp.array([-1, 1])[:, None], jnp.array([0.25, 0.25])[:, None]\n",
    "\n",
    "x_plot = jnp.linspace(-2, 2, 1000)\n",
    "y = gmm(x_plot, norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.plot(x_plot, y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('p(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to drawing samples from the distribution. As it is a Gaussian distribution we can directly sample\n",
    "from it using the build-in sampling methods from JAX. They are found in the `jax.random` namespace. This is\n",
    "again very much like `numpy`, however there is an important difference: instead of a global `RandomState` object\n",
    "JAX uses an approach, based on \"keys\", which can be split locally and are passed to the random sampling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_ = 500_000\n",
    "\n",
    "key = random.key(9834)\n",
    "\n",
    "x_init = sigma * random.normal(key, (2, n_samples_ // 2,)) + mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a histogram of the samples to verify the shape of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.hist(x_init.flatten(), bins=100, density=True, histtype='step', label=\"Initial samples\");\n",
    "ax.set_xlabel('$x_i$')\n",
    "ax.set_ylabel('$p(x_i)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce the actual diffusion process. The diffusion process we choose here consists of two distinct steps or terms:\n",
    "\n",
    "- **Drift**: move current position of sample towards the origin, by multiplying with a constant factor $\\sqrt{1 - \\beta_t}$, this is equivalent to an external force acting on the particle.\n",
    "- **Diffusion**: add noise to the sample, by adding a Gaussian random variable with mean 0 and variance $\\beta_t$, this corresponds to a random motion of the particle (think e.g. [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion))\n",
    "\n",
    "With this specific choice we can achieve that the process finally converges towards a normal Gaussian distribution $\\mathcal{N}(0, 1)$.\n",
    "The choice of the factors given above seems a bit \"magic\" here, but the reasoning is given in the DDPM paper I linked above.\n",
    "\n",
    "For now we can just numerically convince ourselves that this process indeed transforms our two component GMM into a normal Gaussian Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "beta_t = jnp.linspace(0, 1, n_iter)\n",
    "x = x_init.flatten()\n",
    "\n",
    "sample_trace_diffusion_beta = []\n",
    "\n",
    "for idx, beta in enumerate(beta_t):\n",
    "    key, sub_key = random.split(key)\n",
    "    x = jnp.sqrt(1. - beta) * x + beta * random.normal(key=sub_key, shape=x.shape)\n",
    "    sample_trace_diffusion_beta.append(x)\n",
    "\n",
    "sample_trace_diffusion_beta = jnp.stack(sample_trace_diffusion_beta, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the process over time, we can make a histogram of the samples at each point in time `t`. Of course we can use a Python loop for this and call e.g. `np.histogram` and append all results, however this is a good time to introduce a first function transform in JAX, named `jax.vmap`. `jax.vmap` allows to map an arbitrry fucnction over the leading dimension of an array. It is comparable to `np.vectorize`, however the latter basically implements a Python loop, while `jax.vmap` lowers the operation to compiled code and is thus much faster. \n",
    "\n",
    "So now we can use it to map `jnp.histogram` over the time dimension of our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hist = partial(jnp.histogram, bins=100, range=(-3, 3), density=True)\n",
    "\n",
    "batched_histogram = jax.vmap(default_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For covenence we combined this into a little helper function, we can use to see hoe the density chamges with time and plot the trajectory of some example particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(trace, n_traces=5, ax=None, x_min=-3, x_max=3):\n",
    "    \"\"\"Plot distribution at multiple points in time as trace\"\"\"\n",
    "    hist_values, _ = batched_histogram(trace)    \n",
    "\n",
    "    n_iter, n_samples = trace.shape\n",
    "\n",
    "    ax = plt.subplot() or ax\n",
    "    ax.imshow(hist_values.T[:, :], extent=[0, n_iter, x_min, x_max], aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "    # plot some example traces\n",
    "    key = random.PRNGKey(9823)\n",
    "    for idx in random.randint(key, (n_traces,), 0, n_samples):\n",
    "        ax.plot(trace[:, idx])\n",
    "\n",
    "    ax.set_ylim(x_min, x_max)\n",
    "    ax.set_xlabel(\"# Iteration\")\n",
    "    ax.set_ylabel(\"x\")\n",
    "    return ax\n",
    "\n",
    "plot_trace(sample_trace_diffusion_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily convince ourself, that the final (stationary) distribution is indeed a standard Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.hist(sample_trace_diffusion_beta[-1], density=True, bins=50, label=\"Samples\")\n",
    "\n",
    "x_plot = jnp.linspace(-5, 5, 100)\n",
    "ax.plot(x_plot, gaussian(x_plot, norm=1, mu=0, sigma=1), label=\"Unit Gaussian\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"PDF\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Reversing\" Diffusion via Langevin Dynamics\n",
    "\n",
    "Now that we have \"destroyed\" our intial distribution (the two component Gaussian) with the forward diffusion process, wouldn't it be great if we could reverse the process? This would give us a sampling algorithm, that allowed us to sample from an arbitrary distribution. Starting from a normal Gaussian distribution we would transform the samples back into the shape of the intial distribution.\n",
    "\n",
    "It turns out such an algorithm exists, it is a named [\"Langevin Dynamics\"](https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics). However the algorithm requires one additional piece of information:\n",
    "the score function of the target distribution. The score fucntion is defined is defined as the gradient of the log of the distribution.\n",
    "\n",
    "In JAX we can directly get the score function by taking the gradient of the log using `jax.grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gmm(x, norm, mu, sigma):\n",
    "    \"\"\"Log of the GMM\"\"\"\n",
    "    return jnp.log(gmm(x, norm, mu, sigma))\n",
    "\n",
    "gmm_log_part = partial(log_gmm, norm=norm, mu=mu, sigma=sigma) # the partial just simplifies not parsing the parameters later...\n",
    "score_fun = jax.vmap(jax.grad(gmm_log_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "x_plot = jnp.linspace(-2, 2, 1000)\n",
    "ax.plot(x_plot, score_fun(x_plot))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('d/dx log p(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement reverse diffusion process based on \"Langevin Dynamics\". Starting from an initial sample $x_i$, the algorithm looks like this:\n",
    "\n",
    "$\\begin{align} \\mathbf{x}_{i+1} \\gets \\mathbf{x}_i + \\alpha \\nabla_\\mathbf{x} \\log p(\\mathbf{x}) + \\sqrt{2\\alpha}~ \\mathbf{z}_i, \\quad i=0,1,\\cdots, K, \\end{align}$\n",
    "\n",
    "Where $\\mathbf{z}_i \\sim \\mathcal{N}(0, I)$. When $\\alpha \\to 0$ and $K \\to \\infty$, this converges to the target distribution, defined by its score function $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$.\n",
    "\n",
    "While we could implement the algorithm the same way we did above using A Python loop, we make use of another JAX fundemantal, named [`jax.lax.scan`](https://docs.jax.dev/en/latest/_autosummary/jax.lax.scan.html). `lax.scan` allows you scan a function over the leading array axes while carying along a state. It requires us to defined the \"body\" of the loop in a dedicated function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX can natively handle \"structs of arrays\" or \"PyTrees\"\n",
    "SampleArgs = namedtuple(\"Args\", [\"key\", \"idx\", \"x\", \"alpha_0\", \"p_0\"])\n",
    "\n",
    "def sample(score, args, _):\n",
    "    # compute the \"learning rate\" depending on the iteration\n",
    "    alpha = args.alpha_0 * (args.p_0 ** args.idx) ** 2\n",
    "\n",
    "    # sample stochastic update\n",
    "    key, subkey = random.split(args.key)\n",
    "    dx = random.normal(subkey, args.x.shape)\n",
    "\n",
    "    # combine the gradient and the stochastic update\n",
    "    x = args.x + alpha * score(args.x) + jnp.sqrt(2 * alpha) * dx\n",
    "    return SampleArgs(key, args.idx + 1, x, args.alpha_0, args.p_0), x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use that function by defining the initial state first and calling `jax.lax.scan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100_000\n",
    "n_iter = 500\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "init = SampleArgs(\n",
    "    key=key,\n",
    "    idx=0,\n",
    "    x=random.normal(subkey, (n_samples,)),\n",
    "    alpha_0=0.002,\n",
    "    p_0=1.0\n",
    ")\n",
    "\n",
    "result, sample_trace = jax.lax.scan(partial(sample, score_fun), init, length=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace(sample_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah! So we have transformed the samples from a unit Gaussian into the target distribution (the two component Gaussian Mixture we have started with).\n",
    "\n",
    "The beta parametrization has one drawback: we have to compute all the previous steps to get the state of the distribution at time-step t.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
